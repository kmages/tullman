#!/usr/bin/env python3
# -*- coding: utf-8 -*-
from pathlib import Path
from collections import OrderedDict
import os, re, json, unicodedata, requests
from bs4 import BeautifulSoup

BASE     = Path.home() / "tullman"
DATA     = BASE / "data"
CONTENT  = DATA / "content" / "content.jsonl"
UA       = "Mozilla/5.0 (TullmanAI/1.0)"
WIKI_URL = "https://en.wikipedia.org/wiki/Howard_Tullman"

BAD_TERMS  = {'gmail','my thoughts','kmages','ken mages','avatarbuddy','trincity','eyelevel','just ken','newsletter','on infinity'}
GOOD_HINTS = {'tullman','howard tullman','hat ',' 1871 ','kendall college','tribeca flashpoint','chicagoland entrepreneurial center'}

def _norm(s:str)->str:
    if not s: return ""
    s = unicodedata.normalize("NFKD", s)
    return "".join(ch for ch in s if ord(ch) < 0x0300).lower()

def _looks_email(t:str)->bool:
    return bool(re.search(r'(?im)^(from|to|cc|bcc|subject|date):', t or ""))

def _load_corpus(limit:int|None=None)->list[dict]:
    out=[]
    if not CONTENT.exists(): return out
    with open(CONTENT, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            try:
                out.append(json.loads(line))
                if limit and len(out)>=limit: break
            except: pass
    return out

def _ok_public(rec:dict)->bool:
    name=_norm(rec.get("source_name"))
    text=_norm(rec.get("text"))
    if _looks_email(rec.get("text","")): return False
    if any(b in name or b in text for b in BAD_TERMS): return False
    if any(g in name for g in GOOD_HINTS): return True
    if any(g in text for g in GOOD_HINTS): return True
    return False

def _clean_title(t:str|None)->str:
    if not t: return "Source"
    # drop trailing “chunk N”
    t = re.sub(r'\s*-\s*chunk\s*\d+\s*$', '', t, flags=re.I)
    t = re.sub(r'\s*chunk\s*\d+\s*$', '', t, flags=re.I).strip()
    # de-shout: ALL CAPS → Title Case
    if t and t.upper()==t:
        t = t.title()
    return t or "Source"

def _select_chunks(prompt:str, public:bool, k:int=6)->tuple[list[dict],list[dict]]:
    rows=_load_corpus(limit=80000)
    q=_norm(prompt)
    terms=set(re.findall(r"[a-zA-Z]{3,}", q))
    scored=[]
    for r in rows:
        if public and not _ok_public(r): continue
        txt=(r.get("text") or "")
        low=txt.lower()
        boost=5 if ("tullman" in ((r.get("source_name") or "") + " " + low)) else 0
        sc=boost + sum(low.count(t) for t in terms)
        if sc>0: scored.append((sc, r))
    scored.sort(key=lambda x:x[0], reverse=True)
    top=[r for _,r in scored[:k]]

    url_sources=[]; seen=set()
    for r in top:
        u=r.get("url")
        if u and u not in seen:
            url_sources.append({"title":_clean_title(r.get("title") or r.get("source_name") or u), "url":u})
            seen.add(u)
    return top, url_sources

def _facts_from(chunks:list[dict], max_facts:int=8)->str:
    facts=[]; seen=set()
    for r in chunks:
        t=(r.get("text") or "")
        for s in re.split(r'(?<=[.!?])\s+', t)[:2]:
            s=s.strip()
            if not s: continue
            if s.lower().startswith(("by ","photo:","copyright")): continue
            if s in seen: continue
            seen.add(s); facts.append(f"- {s}")
            if len(facts)>=max_facts: break
        if len(facts)>=max_facts: break
    return "\n".join(facts)

def _search_web(query:str, site:str|None=None, top:int=4)->list[dict]:
    q = f"site:{site} {query}" if site else query
    r = requests.post("https://duckduckgo.com/html/", data={"q":q}, headers={"User-Agent":UA}, timeout=8)
    soup=BeautifulSoup(r.text,"lxml")
    items=[]
    for a in soup.select(".result__a")[:top]:
        href=a.get("href"); txt=a.get_text(" ", strip=True)
        if href and txt: items.append({"name":txt, "url":href})
    return items

def _fetch(url:str, max_chars:int=6000)->str:
    try:
        r=requests.get(url, headers={"User-Agent":UA}, timeout=10); r.raise_for_status()
        soup=BeautifulSoup(r.text,"lxml")
        for t in soup(["script","style","noscript"]): t.decompose()
        return soup.get_text(" ", strip=True)[:max_chars]
    except: return ""

def _web_fallback(prompt:str)->tuple[str,list[dict]]:
    texts=[]; srcs=[]
    for site in ["howardtullman.com","inc.com"]:
        for h in _search_web(prompt, site, top=4)[:2]:
            u=h.get("url") or ""; 
            if not u: continue
            txt=_fetch(u)
            if len(txt)>=400: texts.append(txt[:3000]); srcs.append({"title":_clean_title(h.get("name")), "url":u})
    if texts: return " ".join(texts)[:6000], srcs
    for h in _search_web(prompt, None, top=4)[:2]:
        u=h.get("url") or ""; 
        if not u: continue
        txt=_fetch(u)
        if len(txt)>=400: texts.append(txt[:3000]); srcs.append({"title":_clean_title(h.get("name")), "url":u})
    if texts: return " ".join(texts)[:6000], srcs
    return "", []

def _quality_gate(md:str)->bool:
    bad = [
        re.search(r'(?i)#\s*answer', md),
        re.search(r'\bI\s+takes\b|\bI\s+has\b|\bI\s+does\b', md),
        re.search(r'(?i)\bHoward\s+Tullman\b.*\bI\b|\bI\b.*\bHoward\s+Tullman\b', md),
    ]
    return not any(bad)

def _curated_fallback(prompt:str)->str:
    p=_norm(prompt)
    if "who is" in p or "about" in p:
        return ("Hi — I’m Howard Tullman. I’m a Chicago-based entrepreneur, operator, and investor. "
                "I led 1871, helped build Tribeca Flashpoint Academy, and turned around Kendall College. "
                "Earlier I founded or ran CCC Information Services, Tunes.com, and The Cobalt Group. "
                "I mentor founders, invest in scrappy teams, and I believe AI is now table stakes for every business and for individuals.")
    if "proud" in p:
        return ("I’m proudest of the people and platforms we built—1871, the turnarounds at Kendall and Tribeca Flashpoint, "
                "and the founders I’ve mentored. The impact—careers, customers, and execution—outlasts me.")
    if "ai strategy" in p or "need an ai" in p or ("ai" in p and "strategy" in p):
        return ("AI is how you compound advantage—speed, coverage, consistency, and a learning loop from your own work. "
                "Start with one painful process, ship a small assist this quarter, measure, and scale what works. "
                "Keep humans in the loop early, cite your sources, and protect data.")
    return ("Hi — I’m Howard Tullman. Let’s get specific about your goal and constraint so we can ship one small win this quarter.")

def _gpt5_rewrite(question:str, facts:str, prior:str, links:list[dict])->str:
    try:
        import openai
        openai.api_key = os.environ["OPENAI_API_KEY"]
        model = os.getenv("OPENAI_MODEL", "gpt-5-thinking")
        sysmsg = ("You are Howard Tullman. Speak in first person (I, my). "
                  "Be warm, candid, direct, optimistic, and concise. "
                  "Return clean Markdown. Do not add '# Answer'. "
                  "Fix subject–verb agreement; avoid repeating article titles; no third-person self-reference.")
        link_md = "\n".join(f"- {l['title']}: {l['url']}" for l in links) if links else ""
        user = f"QUESTION:\n{question}\n\nFACTS:\n{facts or '(none)'}\n\nCONTEXT:\n{prior or '(none)'}\n\nLINKS (cite if relevant):\n{link_md}"
        resp = openai.ChatCompletion.create(
            model=model,
            messages=[{"role":"system","content":sysmsg},
                      {"role":"user","content":user}],
            temperature=0.2,
        )
        return (resp["choices"][0]["message"]["content"] or "").strip()
    except Exception:
        return _curated_fallback(question)

def _prefer_links(prompt:str, links:list[dict], max_links:int=2)->list[dict]:
    """Prefer Wikipedia for bio-style asks; dedupe; cap; clean titles."""
    p = (prompt or '').lower()
    out = links[:] if links else []
    if any(k in p for k in ('who is','about','bio')) and not any('wikipedia.org' in (l.get('url') or '') for l in out):
        out = [{'title':'Wikipedia: Howard Tullman','url': WIKI_URL}] + out
    seen=set(); clean=[]
    for l in out:
        u=l.get('url')
        if not u or u in seen: continue
        seen.add(u)
        l['title'] = _clean_title(l.get('title') or u)
        clean.append(l)
        if len(clean) >= max_links: break
    return clean

def compose(prompt:str, public:bool=True, prior:str="")->tuple[str,list[dict]]:
    """Compose a first-person answer and URL chips (public)."""
    # 1) JSON-first with strict public filter
    chunks, url_sources = _select_chunks(prompt, public=public, k=6)
    facts = _facts_from(chunks)
    links = url_sources.copy()

    # 2) If no JSON facts (public), try web fallback
    ctx = facts
    if public and not facts:
        ctx_web, links_web = _web_fallback(prompt)
        ctx = ctx_web or ctx
        links = links_web or links
        if not links and re.search(r'(?i)howard\s+tullman', prompt):
            links = [{"title":"Wikipedia: Howard Tullman","url": WIKI_URL}]

    # 3) GPT-5 rewrite → quality gate → curated fallback if needed
    md = _gpt5_rewrite(prompt, ctx, prior, links)
    if not _quality_gate(md):
        md = _gpt5_rewrite(prompt + "\n\nPOLISH:\nFix any verb agreement and remove third-person mentions of my name.", ctx, prior, links)
    if not _quality_gate(md):
        md = _curated_fallback(prompt)

    # 4) Public chips: URLs only, prefer bio link, cap to 1–2
    if public:
    links = [s for s in links if s.get("url")]
    links = _prefer_links(prompt, links, max_links=2)

    if public:
        links = [x for x in links if x.get("url")]
        links = _prefer_links(prompt, links, max_links=2)
return md, links


def _is_bio_prompt(prompt:str)->bool:
    q = (prompt or '').lower()
    return any(k in q for k in ('who is','about','bio','profile','background'))
